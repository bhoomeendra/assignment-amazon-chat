{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493101ef-901b-4913-87b5-e5d5021aa67b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "1. Intilizing the Data and loading Llama 2 piplline for text-genration\n",
    "2. Defining Dataset Class\n",
    "3. Summary Generation\n",
    "4. Evalution\n",
    "5. Results and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9638b5-cfee-4773-bada-e05dd884aff8",
   "metadata": {},
   "source": [
    "### Intilizing the Data and loading Llama 2 piplline for text-genration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e318a3-5987-4bc2-adcb-f1c5f6baa875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "conv_data = pd.read_csv('dataset/100_conversation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7876d3d0-79de-4795-b5e7-15451a3d0489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>conv_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you a fan of Google or Microsoft?\\nBoth ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do you like dance?\\nYes  I do. Did you know Br...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey what's up do use Google very often?I reall...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi!  do you like to dance?\\nI love to dance a ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you like dance?\\nI love it. Did you know Br...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation  conv_id\n",
       "0  Are you a fan of Google or Microsoft?\\nBoth ar...        1\n",
       "1  do you like dance?\\nYes  I do. Did you know Br...        2\n",
       "2  Hey what's up do use Google very often?I reall...        3\n",
       "3  Hi!  do you like to dance?\\nI love to dance a ...        4\n",
       "4  do you like dance?\\nI love it. Did you know Br...        5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee1f614-8ffb-42c1-8537-bbbf1b4fab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cd5d9d16644cf58e0322972645805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" # if you have GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb79dc-2206-4dae-805a-6d55d01fa253",
   "metadata": {},
   "source": [
    "### Defining Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3061565-6351-4026-9d16-bb584178e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# Make the prompt into the standard format\n",
    "class summary_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return f'For the convertation summarize in a paragraph it such that all the\\\n",
    "        important points are covered the conversation is as follows ```{self.data.conversation[idx]}``` \\n Summary:'\n",
    "dataset = summary_dataset(conv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39f20b04-6b92-4c81-85cd-072b3eb5be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d32dc-31c1-41e9-b7b4-5a65db1343cf",
   "metadata": {},
   "source": [
    "### Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51d0ce5a-a94a-4e3e-be2a-c04fa2ac9c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [10:10<00:00,  6.11s/it]\n"
     ]
    }
   ],
   "source": [
    "coversations,summarizations = [],[]\n",
    "for idx in tqdm(range(len(dataset)),desc=\"Generation: \"):\n",
    "    # print(idx)\n",
    "    out = dataset[idx]\n",
    "    \n",
    "    sequences = pipeline(out,\n",
    "                        do_sample=True,\n",
    "                        top_k=10,\n",
    "                        top_p = 0.9,\n",
    "                        temperature = 0.2,\n",
    "                        num_return_sequences=1,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        max_length=2048, # can increase the length of sequence \n",
    "                        )\n",
    "    # ipdb.set_trace()\n",
    "    conv,summ = sequences[0]['generated_text'].split(\"\\n Summary:\")\n",
    "    # print(conv,summ)\n",
    "    coversations.append(conv)\n",
    "    summarizations.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90259e38-0f4d-4c19-a58b-f99df2ebfa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = pd.DataFrame({\"conversations\":coversations,\"summarizations\":summarizations})\n",
    "summ.to_csv(\"dataset/summaries.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79114043-237a-454f-8906-c45bfb11461a",
   "metadata": {},
   "source": [
    "### Evalutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9b9db-b10e-4f86-9771-df1d3eacae77",
   "metadata": {},
   "source": [
    "The challange with evalution is that we do not have ground truth summaries so we have calculated the metrics between the conversation and the generated summaries which is not ideal because they are overlap based metrics i.e rouge and chrf and hence they will suffer but metrics like bertscore would be more meaningful as they evalute sematices similarity between the source and the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0cca37-6b4f-4900-93ea-7f0e3d1096f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f69e9b8-f20a-4c2a-90bc-9b592a5959ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.read_csv(\"dataset/summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dbdff4b-b750-4ea5-ba6a-34891c788dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7afe60-72e2-45ef-b843-d3740fed0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# predictions = [\"hello there\", \"general kenobi\"]\n",
    "# references = [\"hello there\", \"general kenobi\"]\n",
    "def cal():\n",
    "    precision,recall,f1 = 0,0,0\n",
    "    for a,b in tqdm(tab.iterrows()):\n",
    "        # print(b['conversations'],b['summarizations'])\n",
    "        out = bertscore.compute(predictions=[b['summarizations']], references=[b['conversations']], lang=\"en\")\n",
    "        # print(out)\n",
    "        precision += out['precision'][0]\n",
    "        recall += out['recall'][0]\n",
    "        f1 += out['f1'][0]\n",
    "        \n",
    "    print(f\"The bertscore based scores \\n Precision: {precision/len(tab)} \\n Recall: {recall/len(tab)} \\n F1: {f1/len(tab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e2809de-19b9-4f80-97a2-a98cb141e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:06, 15.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bertscore based scores \n",
      " Precision: 0.8656682378053665 \n",
      " Recall: 0.7922304481267929 \n",
      " F1: 0.8272823590040207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da342d23-647c-416a-af8e-cde97cf70eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:01, 94.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chrf score: 18.893789145698715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chrf = load(\"chrf\")\n",
    "def cal():\n",
    "    bleu_s = 0\n",
    "    for a,b in tqdm(tab.iterrows()):\n",
    "        # print(b['conversations'],b['summarizations'])\n",
    "        out = chrf.compute(predictions=[b['summarizations']], references=[b['conversations']])\n",
    "        # print(out)\n",
    "        bleu_s += out['score']\n",
    "        \n",
    "    print(f\"The chrf score: {bleu_s/len(tab)}\")\n",
    "cal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e934d7af-bd3e-4f5e-a298-0e705f927bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29d74f58-7665-4a4b-9573-7f0778c160d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal():\n",
    "    precision,recall,f1 = 0,0,0\n",
    "    for a,b in tqdm(tab.iterrows()):\n",
    "        # print(b['conversations'],b['summarizations'])\n",
    "        out = rouge.compute(predictions=[b['summarizations']], references=[b['conversations']])\n",
    "        # print(out)\n",
    "        precision += out['rouge1']\n",
    "        # recall += out['rouge2']\n",
    "        f1 += out['rougeL']\n",
    "        \n",
    "    print(f\"The Rouge based scores \\n rouge1: {precision/len(tab)} \\n rougeL: {f1/len(tab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ab61626-0ae5-4c58-8633-54961ea07bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:21,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rouge based scores \n",
      " rouge1: 0.24814070160621415 \n",
      " rougeL: 0.16468335879954318\n"
     ]
    }
   ],
   "source": [
    "cal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408ecad-ae3a-4928-a4d5-770b5fe3f9ca",
   "metadata": {},
   "source": [
    "### Results and conclusion\n",
    "\n",
    "LLMs like llama are well suited for the task open ended summarization and with zero and few shot prompting. In the experiment above we have use zero short prompting. As out task is open ended generation and no ground truth summaries are aviable to evalute our generated output. I decided to check the sematice and sentatic similarity between the generating summaries and the referce coversation which was summaried. Below I have reported the metrics for evalution.\n",
    "\n",
    "|Metric|precision|recall|f1|\n",
    "|-|---------|-----|--|\n",
    "|Bertscore|0.8656|0.7922|0.8272|\n",
    "\n",
    "We observe a good bertscore of 0.82 which indicates that the summaries were able to caputre the sematice of the conversations. but the same can not be said for rouge and chrf scores as these are based on n-gram overlap which is not ideal metric to use between the generating summaries and the referce coversation which was summaried.\n",
    "\n",
    "```Rouge1: 0.2481```\n",
    "\n",
    "```RougeL: 0.1646```\n",
    "\n",
    "```CHRF: 18.89```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17e902-fc44-4fa2-8d24-e97f98c0b37e",
   "metadata": {},
   "source": [
    "### Future work \n",
    "1. Few Sort Promptig can be tried to guide the model into generating specific kind of summary \n",
    "2. Summary Generation from GPT-4 and fine tune on llama2 with QLORA this will be one of the ways to distill the knowledge of highly capable model like GPT-4 to a much smaller model for a specific use case and as we are using QLORA we will can use the sample model with just differect adapter weights which are very small. \n",
    "3. Desing better evalutation statergy like NER based overlap between the generation and the converstion and if we have the time and resouces we should do some evalution with humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779aeac7-e70f-49f4-a59f-2bf40ee6c496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
