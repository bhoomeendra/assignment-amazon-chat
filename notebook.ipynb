{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e9642a-05b3-4463-a9cc-ec294c5f3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torcheval.metrics import MulticlassConfusionMatrix,MulticlassF1Score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ipdb\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff977df-cea8-4b5e-9fe1-fe1dba2404b9",
   "metadata": {},
   "source": [
    "# EDA, Data Formating and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80856ac2-a859-45d1-bc45-ec4835ac218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"topical_chat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb607a71-4b37-4576-b70a-c346961d07f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you a fan of Google or Microsoft?</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Both are excellent technology they are helpful...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm not  a huge fan of Google, but I use it a...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Google provides online related services and p...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah, their services are good. I'm just not a...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id                                            message  \\\n",
       "0                1              Are you a fan of Google or Microsoft?   \n",
       "1                1  Both are excellent technology they are helpful...   \n",
       "2                1   I'm not  a huge fan of Google, but I use it a...   \n",
       "3                1   Google provides online related services and p...   \n",
       "4                1   Yeah, their services are good. I'm just not a...   \n",
       "\n",
       "                sentiment  \n",
       "0  Curious to dive deeper  \n",
       "1  Curious to dive deeper  \n",
       "2  Curious to dive deeper  \n",
       "3  Curious to dive deeper  \n",
       "4  Curious to dive deeper  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdea5f8b-a5ca-47ce-9395-2d6dc746ad50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Curious to dive deeper', 'Happy', 'Neutral', 'Surprised',\n",
       "       'Disgusted', 'Sad', 'Fearful', 'Angry'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb91a8db-3dc7-4ba2-8b22-1b848fa23ba6",
   "metadata": {},
   "source": [
    "Impute Null rows which have nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a2030f-5da0-4a2c-ac07-c17f0f55e895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188378"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725c0ac3-cc7b-4106-97da-95f0e5ec881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9816fabb-13b9-495b-91d4-3c9090d468d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188373"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbeed7-9a75-4965-8b33-8da098bcfd91",
   "metadata": {},
   "source": [
    "  We will create 3 differect datasets for the three different task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510b14b-0fed-43ac-8c6b-012f2c0a481e",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e674614d-1c12-4b2f-a27a-75f9d653eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = data[[\"message\",\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c64e80-6aca-4f50-9c34-5fa2d00ae16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_data.to_csv(\"dataset/sentiment_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329dcc73-0f1c-4d3c-a50d-eeb0328e279b",
   "metadata": {},
   "source": [
    "### Summary Generation and QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b6fc15-2a1f-42d4-9979-b8a89a465165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8628\n"
     ]
    }
   ],
   "source": [
    "summ_dataset = []\n",
    "conv_id = []\n",
    "for idx,cov in data.groupby(['conversation_id']):\n",
    "    summ_dataset.append(\"\\n\".join(cov['message']).strip(\"\\n\"))\n",
    "    conv_id.append(idx)\n",
    "    # print(cov)\n",
    "    # out = ''\n",
    "    # for x in cov['message']:\n",
    "    #     if x in not None:\n",
    "    #         out+=x+'\\n'\n",
    "    # summ_dataset.append(out.strip('\\n'))    \n",
    "    # try:\n",
    "    #     summ_dataset.append(\"\\n\".join(cov['message']).strip(\"\\n\"))\n",
    "    # except:\n",
    "    #     print(f\"ERROR: In conversatoin id {idx}\")\n",
    "    #     out = ''\n",
    "    #     for x in cov['message']:\n",
    "    #         out += \"\\n\"+str(x)\n",
    "    #     out.strip('\\n')\n",
    "    #     print(out)\n",
    "print(len(summ_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46ad8916-1597-48cf-a145-7584cc483d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'conversation':summ_dataset[:100] , \"conv_id\":conv_id[:100]}).to_csv('dataset/100_conversation.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff448325-45d2-4f9b-b3cf-2d252ca44609",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'conversation':summ_dataset[:100] , \"conv_id\":conv_id[:100]}).to_csv('dataset/conversation.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770b426d-7f00-4d07-a4de-2aa1d95ea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [ msg.strip(' ') for msg in data['message'] ]\n",
    "# qa_pair = []\n",
    "# idx = 0\n",
    "# while(idx<len(messages)):\n",
    "#     pair = []\n",
    "#     if messages[idx][-1]=='?':\n",
    "#         pair.append(messages[idx])\n",
    "#         pair.append(messages[idx+1])\n",
    "#         qa_pair.append(pair)\n",
    "#         idx+=1\n",
    "#     idx+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55751681-ce26-486b-9185-e4b8e952d228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34303"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(qa_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b15ee60c-956d-46c0-9af3-ffa6765c79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ques , ans = [],[]\n",
    "# for q,a in qa_pair:\n",
    "#     ques.append(q)\n",
    "#     ans.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f54a5dd-e958-4894-8f82-c73326ceb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'Questions':ques,'Answers':ans}).to_csv('dataset/qa_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c0d56-e5dc-41e2-a5b6-12782afda6cd",
   "metadata": {},
   "source": [
    "## Model Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3bea7b-0e4f-40a6-ae61-8803f3f86778",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv(\"dataset/sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61c4651-5ddd-4e12-94cb-053d3acca37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Curious to dive deeper', 'Happy', 'Neutral', 'Surprised',\n",
       "       'Disgusted', 'Sad', 'Fearful', 'Angry'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f372c2-2e46-4c3c-9bb5-94df8927eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "str2cls = {\n",
    " 'Curious to dive deeper':0, \n",
    " 'Happy':1, \n",
    " 'Neutral':2, \n",
    " 'Surprised':3,\n",
    " 'Disgusted':4, \n",
    " 'Sad':5,\n",
    " 'Fearful':6,\n",
    " 'Angry':7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d13569-ba57-408a-9ff6-c5879ace1ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_data['sentiment'] = sentiment_data['sentiment'].replace(str2cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb62bf00-086b-4525-8f33-5843cbce3b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c7f07e-cd41-462b-8c8d-c22771e0322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    80887\n",
       "2    41367\n",
       "3    30637\n",
       "1    29615\n",
       "5     2533\n",
       "4     1432\n",
       "6     1026\n",
       "7      876\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e2b0c44-097f-4a21-a9cf-6084d8347cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_dist = (sentiment_data['sentiment'].value_counts()/len(sentiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f35823e-9273-4fa8-aecd-7f6c6c9dfe69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.429398\n",
       "2    0.219602\n",
       "3    0.162640\n",
       "1    0.157215\n",
       "5    0.013447\n",
       "4    0.007602\n",
       "6    0.005447\n",
       "7    0.004650\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef38256-5f01-4434-9396-8574f4f95ef1",
   "metadata": {},
   "source": [
    "The classes are higly imbalanced so some oversampling might be needed to make the data balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a791b329-533d-4a33-b823-857b2a5bf758",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights = cls_dist**(-1)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b2335b3-63a6-4fa1-beb9-4f75afc302b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1883.73\n",
       "2    1883.73\n",
       "3    1883.73\n",
       "1    1883.73\n",
       "5    1883.73\n",
       "4    1883.73\n",
       "6    1883.73\n",
       "7    1883.73\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_weights * sentiment_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "240ab8e6-808a-4ab4-b828-b4159a54ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights = { key:val for key,val in zip(cls_weights.index,cls_weights) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8eb47d5-1f31-471c-833a-e68f15a91812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.023288414701991663,\n",
       " 2: 0.045537022264123574,\n",
       " 3: 0.06148545875901688,\n",
       " 1: 0.0636072936012156,\n",
       " 5: 0.7436754836162653,\n",
       " 4: 1.315453910614525,\n",
       " 6: 1.8359941520467837,\n",
       " 7: 2.150376712328767}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761b87b0-d4c7-487b-9c8c-2f208847c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskADataset(Dataset):\n",
    "    \n",
    "    def over_sampling(self,data_base,weights):\n",
    "        # Oversampling is done so that the dataset is balanced\n",
    "        data = []\n",
    "        for idx,gp in data_base.groupby(['sentiment']):\n",
    "            out = gp.sample(frac=weights[idx], replace=True,random_state=43)\n",
    "            data.append(out)\n",
    "        data = pd.concat(data)\n",
    "        print(data.describe())\n",
    "        return data\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_len:int, weights={i:1 for i in range(8)}):\n",
    "        self.data = self.over_sampling(data,weights)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.iloc[index]\n",
    "        text = data.message\n",
    "        label = data.sentiment\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60da71ac-e47e-4ed8-99db-2a6191849f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskA_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(TaskA_Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.L1 = nn.Linear(768, 200)\n",
    "        self.L2 = nn.Linear(200,25)\n",
    "        self.classifier = nn.Linear(25,num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs = self.L1(outputs.pooler_output)\n",
    "        outputs = self.L2(outputs)\n",
    "        class_weights = self.classifier(outputs)\n",
    "        probs = self.softmax(class_weights)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979c859b-5d6a-41d8-b0b8-205b1776e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605cb03-c9be-4eb6-b626-2af1935a0db7",
   "metadata": {},
   "source": [
    "## Need to look at this code and modify this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74a9e622-3a53-473a-9fd8-f223d621aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model,criterion,metrics,th,val_dataloader,min_val_loss=-1):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    total_val_loss = 0\n",
    "    for batch_idx,data in tqdm(enumerate(val_dataloader),total = len(val_dataloader)):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "        output = model(input_ids, attention_mask)\n",
    "        pred = torch.zeros(len(label))\n",
    "        pred[output[:,1]>th] = 1\n",
    "        loss = criterion(output,label)\n",
    "        pred = pred.to(torch.int64)\n",
    "        label = label.to(torch.int64)\n",
    "        label = label.to('cpu')\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        for m in metrics:\n",
    "            m.update(pred,label)\n",
    "        \n",
    "        total_val_loss+=loss.item()\n",
    "    val_loss = total_val_loss/len(val_dataloader)\n",
    "    if val_loss<min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(),\"./model_bert_freeze\")\n",
    "        print(\"Model Saved\")\n",
    "\n",
    "    print(\"Macro F1: \",metrics[0].compute())\n",
    "    print(\"Micro F1: \",metrics[1].compute())\n",
    "    print(\"Confusion Matric: \\n\",metrics[2].compute())\n",
    "    if len(metrics)>3:\n",
    "        print(\"Accuracy: \",metrics[3].compute())\n",
    "    for m in metrics:\n",
    "            m.reset()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_model(model,optimizer,criterion,scheduler,num_epoch,F1_macro_metric,F1_micro_metric,confusion_matrix,th,val_dataloader):\n",
    "    min_val_loss = 1000\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        tot_loss = 0\n",
    "        for batch_idx, data in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            label = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item()\n",
    "        \n",
    "        val_loss = val_model( model,\n",
    "                              criterion,\n",
    "                              F1_macro_metric,\n",
    "                              F1_micro_metric,\n",
    "                              confusion_matrix,\n",
    "                              th,\n",
    "                              val_dataloader,\n",
    "                              min_val_loss)\n",
    "        \n",
    "        min_val_loss = min(min_val_loss,val_loss)\n",
    "        print(f\"Epoch Loss : {tot_loss/len(train_loader)} Val Loss: {val_loss}\")\n",
    "        scheduler.step(tot_loss/len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a48bf9-2baa-478d-a638-443c37617be8",
   "metadata": {},
   "source": [
    "## Split the data into train val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "350743e6-daa7-4dd1-82b2-f2f484c841b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,val = train_test_split(sentiment_data,test_size=0.1,stratify=sentiment_data['sentiment'],random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "426ab342-9aee-4294-8199-98394f4a47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.429398\n",
       "2    0.219601\n",
       "3    0.162639\n",
       "1    0.157212\n",
       "5    0.013449\n",
       "4    0.007603\n",
       "6    0.005444\n",
       "7    0.004654\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef06aa3f-1c98-4e07-912e-babf8b6c3779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.429398\n",
       "2    0.219609\n",
       "3    0.162650\n",
       "1    0.157235\n",
       "5    0.013430\n",
       "4    0.007591\n",
       "6    0.005468\n",
       "7    0.004618\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['sentiment'].value_counts()/len(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a048576-b10a-433e-bc28-4144e281368b",
   "metadata": {},
   "source": [
    "## Setup the model and init weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a2d847-a8ef-429f-95c2-76746463e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d37bf16-071e-4a94-8ee0-e8b6efc413db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of message:  227\n"
     ]
    }
   ],
   "source": [
    "max_len = max( [ len(i.split()) for i in train[\"message\"] ] )+100\n",
    "print(\"Max length of message: \", max_len)\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "nclasses = len(cls_weights)\n",
    "model = TaskA_Model(model_name,num_classes=nclasses).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7f524a-a665-4ae8-b902-34c357fda1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model exists and loaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('model_bert_freeze'):\n",
    "    model.load_state_dict(torch.load('model_bert_freeze'))\n",
    "    print(\"Pretrained model exists and loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37e89e-6404-48ce-8f3c-b431182bf1b1",
   "metadata": {},
   "source": [
    "## If only the linear layer traning is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adcaf83a-5393-4f94-b79b-dfbb6c6967d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in model.bert.parameters():\n",
    "#     params.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c049b8-8a68-4a64-8524-94897a4e99a9",
   "metadata": {},
   "source": [
    "## Init the params for bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ba26c67-0ef2-45f3-92e1-111b249b3deb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sentiment\n",
      "count  13564.000000\n",
      "mean       3.500664\n",
      "std        2.291469\n",
      "min        0.000000\n",
      "25%        2.000000\n",
      "50%        4.000000\n",
      "75%        6.000000\n",
      "max        7.000000\n",
      "          sentiment\n",
      "count  18838.000000\n",
      "mean       1.247054\n",
      "std        1.350861\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        2.000000\n",
      "max        7.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:42<00:00,  2.48it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Macro F1:  tensor(0.1210)\n",
      "Micro F1:  tensor(0.4477)\n",
      "Confusion Matric: \n",
      " tensor([[7.4670e+03, 6.2200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.9950e+03, 9.6700e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.5570e+03, 5.8000e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.8190e+03, 2.4500e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.4200e+02, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.4400e+02, 9.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+02, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.6000e+01, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 0 Loss : 1.8296617986458652 Val Loss: 1.8824729389164767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:32<00:00,  2.55it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:02<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Macro F1:  tensor(0.1236)\n",
      "Micro F1:  tensor(0.4385)\n",
      "Confusion Matric: \n",
      " tensor([[6.9060e+03, 1.1830e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.6070e+03, 1.3550e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.1230e+03, 1.0140e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.5580e+03, 5.0600e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3500e+02, 8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.3200e+02, 2.1000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.7000e+01, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.4000e+01, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 1 Loss : 1.7351704210324108 Val Loss: 1.8813569435441837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:37<00:00,  2.51it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  tensor(0.1196)\n",
      "Micro F1:  tensor(0.4490)\n",
      "Confusion Matric: \n",
      " tensor([[7.5930e+03, 4.9600e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.0960e+03, 8.6600e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.6550e+03, 4.8200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.8820e+03, 1.8200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.4100e+02, 2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.4600e+02, 7.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+02, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.6000e+01, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 2 Loss : 1.666755938305045 Val Loss: 1.8946822168264406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:36<00:00,  2.52it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Macro F1:  tensor(0.1230)\n",
      "Micro F1:  tensor(0.4427)\n",
      "Confusion Matric: \n",
      " tensor([[7.1760e+03, 9.1300e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.7990e+03, 1.1630e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.4550e+03, 6.8200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.6840e+03, 3.8000e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3800e+02, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.4700e+02, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+02, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.1000e+01, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 3 Loss : 1.6277901549946587 Val Loss: 1.8681000497426161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:35<00:00,  2.52it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Macro F1:  tensor(0.1241)\n",
      "Micro F1:  tensor(0.4419)\n",
      "Confusion Matric: \n",
      " tensor([[7.0400e+03, 1.0490e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.6770e+03, 1.2850e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.3010e+03, 8.3600e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.5970e+03, 4.6700e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3800e+02, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.3900e+02, 1.4000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.8000e+01, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.1000e+01, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 4 Loss : 1.59918227566863 Val Loss: 1.857697754238174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:37<00:00,  2.51it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  tensor(0.1221)\n",
      "Micro F1:  tensor(0.4415)\n",
      "Confusion Matric: \n",
      " tensor([[7.1750e+03, 9.1400e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.8200e+03, 1.1420e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.3950e+03, 7.4200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.7230e+03, 3.4100e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3700e+02, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.4700e+02, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.8000e+01, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.1000e+01, 6.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 5 Loss : 1.5805244509060428 Val Loss: 1.8835942552935692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:35<00:00,  2.53it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Macro F1:  tensor(0.1204)\n",
      "Micro F1:  tensor(0.4402)\n",
      "Confusion Matric: \n",
      " tensor([[7.2500e+03, 8.3900e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.9200e+03, 1.0420e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.5810e+03, 5.5600e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.7220e+03, 3.4200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3900e+02, 4.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.5000e+02, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0200e+02, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.6000e+01, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 6 Loss : 1.5607046316254813 Val Loss: 1.8541353630290978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:33<00:00,  2.54it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  tensor(0.1220)\n",
      "Micro F1:  tensor(0.4355)\n",
      "Confusion Matric: \n",
      " tensor([[6.9630e+03, 1.1260e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.7210e+03, 1.2410e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.3760e+03, 7.6100e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.6380e+03, 4.2600e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.4000e+02, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.4500e+02, 8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.8000e+01, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [7.9000e+01, 8.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 7 Loss : 1.5596136052934628 Val Loss: 1.8680588649570031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:37<00:00,  2.51it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  tensor(0.1227)\n",
      "Micro F1:  tensor(0.4434)\n",
      "Confusion Matric: \n",
      " tensor([[7.2210e+03, 8.6800e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.8310e+03, 1.1310e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.4980e+03, 6.3900e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.6770e+03, 3.8700e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3800e+02, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.4400e+02, 9.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.9000e+01, 4.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.6000e+01, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 8 Loss : 1.5426804930815157 Val Loss: 1.8698728844548729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 848/848 [05:37<00:00,  2.51it/s]\n",
      "100%|████████████████████████| 1178/1178 [03:01<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  tensor(0.1206)\n",
      "Micro F1:  tensor(0.4364)\n",
      "Confusion Matric: \n",
      " tensor([[7.0870e+03, 1.0020e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.8280e+03, 1.1340e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.4480e+03, 6.8900e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.6920e+03, 3.7200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.4100e+02, 2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.4900e+02, 4.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+02, 3.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.3000e+01, 4.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]])\n",
      "Epoch: 9 Loss : 1.5360625124764893 Val Loss: 1.9004575205375465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',threshold=1e-3)\n",
    "num_epoch = 10\n",
    "th = 0.5\n",
    "F1_macro_metric = MulticlassF1Score(average=\"macro\", num_classes=nclasses)\n",
    "F1_micro_metric = MulticlassF1Score(average=\"micro\", num_classes=nclasses)\n",
    "confusion_matrix = MulticlassConfusionMatrix(num_classes=nclasses)\n",
    "\n",
    "train_dataset = TaskADataset(train, tokenizer, max_len,weights=cls_weights)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = TaskADataset(val,tokenizer,max_len)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=16)\n",
    "\n",
    "\n",
    "train_model(model = model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            scheduler=scheduler,\n",
    "            num_epoch=num_epoch,\n",
    "            F1_macro_metric=F1_macro_metric,\n",
    "            F1_micro_metric=F1_micro_metric,\n",
    "            confusion_matrix=confusion_matrix,\n",
    "            th=th,\n",
    "            val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c1205-73b6-483b-b150-13a639fa94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change bert to S-sbert roberta large or some bigger mode\n",
    "# Llama 2 find tune for classification\n",
    "\n",
    "# F1 Macro , F1-Micro,  Confusion_matrix, weigted F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca88d6-bb59-4d69-8996-6409ae8ffea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future work\n",
    "1. Classificatioasn io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc07e01-afba-44ad-8189-734c53f07b5d",
   "metadata": {},
   "source": [
    "## Model Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4043f63a-a822-44ec-95cd-37372762138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mistral 7B with prompt tuning with conversation \n",
    "## Use sometthing like bert score between the conversation and the summary \n",
    "## Some modified form of blue score or other can be used (where we remove the stopword )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a45ba4-9193-4ed4-b314-72bc2ed03013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conv_data = pd.read_csv('dataset/100_conversation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d31ceee-c54b-4a81-a09c-b8d5435fbae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>conv_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you a fan of Google or Microsoft?\\nBoth ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do you like dance?\\nYes  I do. Did you know Br...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey what's up do use Google very often?I reall...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi!  do you like to dance?\\nI love to dance a ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you like dance?\\nI love it. Did you know Br...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        conversation  conv_id\n",
       "0  Are you a fan of Google or Microsoft?\\nBoth ar...        1\n",
       "1  do you like dance?\\nYes  I do. Did you know Br...        2\n",
       "2  Hey what's up do use Google very often?I reall...        3\n",
       "3  Hi!  do you like to dance?\\nI love to dance a ...        4\n",
       "4  do you like dance?\\nI love it. Did you know Br...        5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba3ad8-d61c-4aa9-97ee-11bd84a4edb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10eb33-3cf0-4acc-ad58-f1cef90ffa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe617c2-71cb-4a84-84fc-61d34d44423f",
   "metadata": {},
   "source": [
    "## Model QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ce7841a-4efe-4e96-abae-2f838bb2e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT-2 Base / Llama 2 Lora fine tune on the qa pairs\n",
    "## Use something like rouge , Bleu to evaluete the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0723f74-6ade-4f2b-af9f-5a0eea1bb622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
